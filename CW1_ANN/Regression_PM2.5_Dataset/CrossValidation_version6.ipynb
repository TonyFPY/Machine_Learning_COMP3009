{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CrossValidation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9f2ZYHw4O2l",
        "outputId": "44ae7246-ddbb-46b7-cdb8-8832beffc009",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "TensorFlow 1.x selected.\n",
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GBZvZEb4sSF",
        "outputId": "a398451e-bdd0-44df-8fc9-d2277c7c42a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "import time\n",
        "from matplotlib import pyplot\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "import random\n",
        "\n",
        "# read data from csv file and process X, y\n",
        "def data_loader(data_path):\n",
        "\t\t# load raw data\n",
        "\t\tdf = pd.read_csv(data_path)\n",
        "\n",
        "\t\t# clean data with missing values          \n",
        "\t\t'''drop the rows directly -> mess up the order\n",
        "\t\t\tfirst 24 rows have pm2.5 value that is NaN -> discard\n",
        "\t\t\telse: forward filling'''\n",
        "\t\tdf = df[24:].fillna(method='ffill')\n",
        "\n",
        "\t\t# integrate 'year', 'month', 'day', 'hour', 'No' as one attribute\n",
        "\t\tdf['time'] = df.apply(lambda x : datetime.datetime(year=x['year'], month=x['month'], day=x['day'], hour=x['hour']), axis=1)\n",
        "\t\tdf.drop(columns=['year', 'month', 'day', 'hour', 'No'], inplace=True)\n",
        "\t\tdf = df.set_index('time')\n",
        "\n",
        "\t\t# expand \"cbwd\" into 4 attributes 'SE', 'cv', 'NW', 'NE'\n",
        "\t\tdf = df.join(pd.get_dummies(df['cbwd'])) # one-hot encoding\n",
        "\t\tdel df['cbwd']\n",
        "\n",
        "\t\t# df = df.drop('cbwd', axis = 1) # remove the attribute temporarily\n",
        "\n",
        "\t\tX = df.iloc[:,1:].astype(np.float32)\n",
        "\t\ty = df['pm2.5'].astype(np.float32).to_frame()\n",
        "\t\n",
        "\t\treturn X, y\n",
        "  \n",
        "# Z-score normalisation\n",
        "def data_normalisation(train, test):\n",
        "\t\tX_train, y_train = train[0], train[1]\n",
        "\t\tX_test, y_test = test[0], test[1]\n",
        "\n",
        "\t\tmean_x, std_x = X_train.mean(axis=0), X_train.std(axis=0)\n",
        "\t\tmean_y, std_y = y_train.mean(axis=0), y_train.std(axis=0)\n",
        "\t\n",
        "\t\t# Use the mean & std of train. Since there's no way for us to know the future.\n",
        "\t\t# (X_train, y_train) = ((X_train-mean_x)/std_x, (y_train-mean_y)/std_y)\n",
        "\t\t# (X_val, y_val) = ((X_val-mean_x)/std_x, (y_val-mean_y)/std_y) \n",
        "\t\t# (X_test, y_test) = ((X_test-mean_x)/std_x, (y_test-mean_y)/std_y)\n",
        "\t\t(X_train, y_train) = ((X_train-mean_x)/std_x, np.log(np.asarray(y_train) + 1))\n",
        "\t\t(X_test, y_test) = ((X_test-mean_x)/std_x, np.log(np.asarray(y_test) + 1))\n",
        "\t\t# (X_train, y_train) = ((X_train-mean_x)/std_x, y_train)\n",
        "\t\t# (X_val, y_val) = ((X_val-mean_x)/std_x, y_val) \n",
        "\t\t# (X_test, y_test) = ((X_test-mean_x)/std_x, y_test)\n",
        "\n",
        "\t\treturn (X_train, y_train),(X_test, y_test)\n",
        "\n",
        "data_path = \"/content/drive/My Drive/Colab Notebooks/PM2dot5.csv\"\n",
        "X, y = data_loader(data_path)\n",
        "\n",
        "#X_data = X.values\n",
        "#y_data = y.values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.1, shuffle=True)\n",
        "\n",
        "(X_train, y_train),(X_test, y_test) = data_normalisation((X_train, y_train),(X_test, y_test))\n",
        "\n",
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape\n",
        "#X_data.shape, y_data.shape"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((39420, 10), (39420, 1), (4380, 10), (4380, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJfnChpY57h7"
      },
      "source": [
        "#Network parameters\n",
        "n_input = 10 \n",
        "n_hidden1, n_hidden2,n_hidden3 = 20, 10, 5\n",
        "n_output = 1\n",
        "\n",
        "#Defining the input and the output\n",
        "X_p = tf.placeholder(\"float\", [None, n_input], name='X_p') \n",
        "Y_p = tf.placeholder(\"float\", [None, n_output], name='Y_p')\n",
        "\n",
        "# forward\n",
        "def multilayer_perceptron1(input_d):\n",
        "    #DEFINING WEIGHTS AND BIASES\n",
        "    b1 = tf.Variable(tf.random_normal([n_hidden1])) #Biases first hidden layer\n",
        "    b2 = tf.Variable(tf.random_normal([n_output]))  #Biases output layer\n",
        "    \n",
        "    w1 = tf.Variable(tf.random_normal([n_input, n_hidden1]))   #Weights connecting input layer with first hidden layer\n",
        "    w2 = tf.Variable(tf.random_normal([n_hidden1, n_output])) #Weights connecting first hidden layer with second hidden layer \n",
        "\n",
        "    layer_1 = tf.nn.leaky_relu(tf.add(tf.matmul(input_d, w1), b1), alpha=0.01) #Task of neurons of first hidden layer\n",
        "    out_layer = tf.add(tf.matmul(layer_1, w2),b2)            #Task of neurons of output layer\n",
        "    \n",
        "    return out_layer\n",
        "def multilayer_perceptron2(input_d):\n",
        "#DEFINING WEIGHTS AND BIASES\n",
        "    b1 = tf.Variable(tf.random_normal([n_hidden1])) #Biases first hidden layer\n",
        "    b2 = tf.Variable(tf.random_normal([n_hidden2])) #Biases second hidden layer\n",
        "    b3 = tf.Variable(tf.random_normal([n_output]))  #Biases output layer\n",
        "    \n",
        "    w1 = tf.Variable(tf.random_normal([n_input, n_hidden1]))   #Weights connecting input layer with first hidden layer\n",
        "    w2 = tf.Variable(tf.random_normal([n_hidden1, n_hidden2])) #Weights connecting first hidden layer with second hidden layer \n",
        "    w3 = tf.Variable(tf.random_normal([n_hidden2, n_output]))\n",
        "\n",
        "    layer_1 = tf.nn.leaky_relu(tf.add(tf.matmul(input_d, w1), b1), alpha=0.01) #Task of neurons of first hidden layer\n",
        "    layer_2 = tf.nn.leaky_relu(tf.add(tf.matmul(layer_1, w2), b2), alpha=0.01) #Task of neurons of second hidden layer\n",
        "    out_layer = tf.add(tf.matmul(layer_2, w3),b3)            #Task of neurons of output layer\n",
        "    return out_layer\n",
        "def multilayer_perceptron3(input_d):\n",
        "#DEFINING WEIGHTS AND BIASES\n",
        "    b1 = tf.Variable(tf.random_normal([n_hidden1])) #Biases first hidden layer\n",
        "    b2 = tf.Variable(tf.random_normal([n_hidden2])) #Biases second hidden layer\n",
        "    b3 = tf.Variable(tf.random_normal([n_hidden3])) #Biases third hidden layer\n",
        "    b4 = tf.Variable(tf.random_normal([n_output]))  #Biases output layer\n",
        "    \n",
        "    w1 = tf.Variable(tf.random_normal([n_input, n_hidden1]))   #Weights connecting input layer with first hidden layer\n",
        "    w2 = tf.Variable(tf.random_normal([n_hidden1, n_hidden2])) #Weights connecting first hidden layer with second hidden layer \n",
        "    w3 = tf.Variable(tf.random_normal([n_hidden2, n_hidden3])) #Weights connecting second hidden layer with third layer\n",
        "    w4 = tf.Variable(tf.random_normal([n_hidden3, n_output]))  #Weights connecting third hidden layer with output layer\n",
        "\n",
        "\n",
        "    layer_1 = tf.nn.leaky_relu(tf.add(tf.matmul(input_d, w1), b1), alpha=0.01) #Task of neurons of first hidden layer\n",
        "    layer_2 = tf.nn.leaky_relu(tf.add(tf.matmul(layer_1, w2), b2), alpha=0.01) #Task of neurons of second hidden layer\n",
        "    layer_3 = tf.nn.leaky_relu(tf.add(tf.matmul(layer_2, w3), b3), alpha=0.01) #Task of neurons of thrid layer\n",
        "    out_layer = tf.add(tf.matmul(layer_3, w4),b4)            #Task of neurons of output layer\n",
        "    return out_layer\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eoe6lo1F6D-k",
        "outputId": "5fecbeb3-1551-47e8-a119-dddbe14a96a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        }
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "kf = KFold(n_splits=10)\n",
        "\n",
        "train_x = []\n",
        "test_x = []\n",
        "train_y = []\n",
        "test_y = []\n",
        "\n",
        "\n",
        "number_epochs = 20000\n",
        "select_epoch_from = 1000\n",
        "batch_iteration = 0\n",
        "\n",
        "train_loss = 0\n",
        "val_loss = 0\n",
        "\n",
        "# record the train losses for the total 10 fold models\n",
        "kfold_train_losses = []\n",
        "# record the validate losses for the total 10 fold models\n",
        "kfold_val_losses = []\n",
        "\n",
        "hyperParameters = {'lr':[0.001,0.003,0.005,0.007],'batch_size':[64,128,256],'type_loss_func':[\"MSE\",\"MAE\",\"MSLE\"],'type_optimizer':[\"GradientDescent\",\"RMSProp\",\"Momentum\",\"Adam\"],'neural_network_structure':[\"multilayer_perceptron1\",\"multilayer_perceptron2\",\"multilayer_perceptron3\"]}\n",
        "\n",
        "rmses = []\n",
        "bset_lr = 0\n",
        "best_batchsize = 0\n",
        "best_loss_func = \"\"\n",
        "best_type_optimizer = \"\"\n",
        "best_neural_network_structure = \"\"\n",
        "best_training_rmse = 0\n",
        "best_validation_rmse = 0\n",
        "\n",
        "start_time = time.clock()\n",
        "with tf.Session() as sess: \n",
        "    for lr in hyperParameters['lr']:\n",
        "        for batch_size in hyperParameters['batch_size']:\n",
        "            for type_loss_func in hyperParameters['type_loss_func']:\n",
        "                for type_optimizer in hyperParameters['type_optimizer']:\n",
        "                    for neural_network_structure in hyperParameters['neural_network_structure']:\n",
        "                        fold = 0\n",
        "                        for train_index, val_index in kf.split(X_train, y_train):\n",
        "                            if neural_network_structure == \"multilayer_perceptron1\":\n",
        "                               neural_network = multilayer_perceptron1(X_p)\n",
        "                            elif neural_network_structure == \"multilayer_perceptron2\":\n",
        "                                 neural_network = multilayer_perceptron2(X_p)\n",
        "                            elif neural_network_structure == \"multilayer_perceptron3\":\n",
        "                                 neural_network = multilayer_perceptron3(X_p)\n",
        "                            mse_1 = tf.keras.losses.MeanSquaredError()\n",
        "                            RMSE_pre = mse_1(neural_network,Y_p)\n",
        "                            eval_RMSE = tf.cast(tf.sqrt(RMSE_pre),dtype=tf.float32)\n",
        "                            if type_loss_func == \"MSE\":\n",
        "                               mse = tf.keras.losses.MeanSquaredError()\n",
        "                               loss_func = mse(neural_network,Y_p)\n",
        "                            elif type_loss_func == \"MAE\":\n",
        "                                 mae = tf.keras.losses.MeanAbsoluteError()\n",
        "                                 loss_func = mse(neural_network,Y_p)\n",
        "                            elif type_loss_func == \"MSLE\":\n",
        "                                 msle = tf.keras.losses.MeanSquaredLogarithmicError()()\n",
        "                                 loss_func = msl(neural_network,Y_p)\n",
        "                            # define optimizer\n",
        "                            if type_optimizer == \"GradientDescent\":\n",
        "                               optimizer = tf.train.GradientDescentOptimizer(lr).minimize(loss_func)\n",
        "                            elif type_optimizer == \"RMSProp\":\n",
        "                                 optimizer = tf.train.RMSPropOptimizer(lr).minimize(loss_func)\n",
        "                            elif type_optimizer == \"Momentum\":\n",
        "                                 optimizer = tf.train.MomentumOptimize(lr).minimize(loss_func)\n",
        "                            elif type_optimizer == \"Adam\":\n",
        "                                 optimizer = tf.train.AdamOptimizer(lr).minimize(loss_func)\n",
        "                            # initializaing variables\n",
        "                            init = tf.global_variables_initializer()\n",
        "                            sess.run(init)\n",
        "                            \n",
        "                            fold = fold + 1\n",
        "                            train_x = X_train[train_index]\n",
        "                            train_y = y_train[train_index]\n",
        "                            val_x = X_train[val_index]\n",
        "                            val_y = y_train[val_index]\n",
        "                            batch_iteration = int(len(train_x) / batch_size) + 1\n",
        "                \n",
        "                            print(\"Fold %s, lr: %s, batch_size: %s, type_loss_func: %s, type_optimizer: %s, neural_network_structure: %s\" % (fold,lr,batch_size,type_loss_func,type_optimizer,neural_network_structure))\n",
        "                \n",
        "                            train_losses = []\n",
        "                            val_losses = []\n",
        "                            epochs = []\n",
        "                            for epoch in range(1,number_epochs+1):\n",
        "          \n",
        "                                n = epoch % batch_iteration\n",
        "                                s = 0 + batch_size * n\n",
        "                                if n == 0:\n",
        "                                   e = len(train_x)\n",
        "                                else:\n",
        "                                    e = batch_size + batch_size * n\n",
        "                  \n",
        "                                batch_X = np.array(train_x[s:e])\n",
        "                                batch_y = np.array(train_y[s:e])\n",
        "\n",
        "                                _, train_loss = sess.run([optimizer, loss_func], feed_dict={X_p: batch_X, Y_p: batch_y})\n",
        "                                val_loss = loss_func.eval({X_p: val_x, Y_p: val_y})\n",
        "\n",
        "                                if epoch >= select_epoch_from and epoch % 100 == 0:\n",
        "                                   train_losses.append(train_loss)\n",
        "                                   val_losses.append(val_loss)\n",
        "                                   epochs.append(epoch)\n",
        "\n",
        "                                #Display the epoch\n",
        "                                if epoch % 1000 == 0:\n",
        "                                   print(\"Epoch: %d, \"\n",
        "                                         \"Training loss: %s, \"\n",
        "                                         \"Validation loss: %s, \" % (epoch, train_loss, val_loss))\n",
        "                \n",
        "                            pyplot.plot(epochs, train_losses, 'r', label='Training loss', linewidth=1, linestyle='-')\n",
        "                            pyplot.plot(epochs, val_losses, 'b', label='Validation loss', linewidth=3, linestyle='-.')\n",
        "                            pyplot.xlabel('Epoch') \n",
        "                            pyplot.ylabel('Loss')\n",
        "                            pyplot.title('Training and validation loss')\n",
        "                            pyplot.legend()\n",
        "                            pyplot.figure()\n",
        "                            pyplot.show()\n",
        "                            pyplot.ioff()\n",
        "\n",
        "                            kfold_train_RMSE = sess.run(eval_RMSE,feed_dict={X_p:train_x, Y_p:train_y})\n",
        "                            kfold_val_RMSE = sess.run(eval_RMSE,feed_dict={X_p:val_x, Y_p:val_y})\n",
        "                      \n",
        "                            print(\"Final Training loss: %s, \"\n",
        "                                  \"Final Validation loss: %s, \" \n",
        "                                   \"Training RMSE: %s, \"\n",
        "                                   \"Validation RMSE: %s\"% (train_loss, val_loss, kfold_train_RMSE, kfold_val_RMSE )) \n",
        "                            print(\"-------------------------------------------------------------------------------------\\n\\n\")\n",
        "\n",
        "                            kfold_train_losses.append(kfold_train_RMSE)\n",
        "                            kfold_val_losses.append(kfold_val_RMSE)\n",
        "  \n",
        "                            if fold == 10:\n",
        "                               # average train RMSE for the 10 folds\n",
        "                               average_train_rmse_score_10fold = np.mean(kfold_train_losses)\n",
        "                               # average validate RMSE for the 10 folds\n",
        "                               average_val_rmse_score_10fold = np.mean(kfold_val_losses)\n",
        "                               print(\"For the model with learning rate: %s, batch size: %s, type_optimizer: %s, neural_network_structure: %s \\n\" % (lr,batch_size,type_optimizer,neural_network_structure))\n",
        "                               print(\"Average Training RMSE for the 10 folds: %s \\n\" % str(average_train_rmse_score_10fold))\n",
        "                               print(\"Average Validation RMSE for the 10 folds: %s \\n\\n\" % str(average_val_rmse_score_10fold))\n",
        "                               print(\"-------------------------------------------------------------------------------------\\n\\n\")\n",
        "                               if lr == 0.001 and batch_size == 64 and type_loss_func == \"MSE\" and type_optimizer == \"GradientDescent\" and neural_network_structure == \"multilayer_perceptron1\":\n",
        "                                  #best_training_rmse = average_train_rmse_score_10fold\n",
        "                                  best_validation_rmse = average_val_rmse_score_10fold\n",
        "                                  best_lr = 0.001\n",
        "                                  best_batchsize = 64\n",
        "                                  best_loss_func = \"MSE\"\n",
        "                                  best_type_optimizer = \"GradientDescent\"\n",
        "                                  best_neural_network_structure = \"multilayer_perceptron1\"\n",
        "                               else: \n",
        "                                   if average_val_rmse_score_10fold < best_validation_rmse:\n",
        "                                      #best_training_rmse = average_train_rmse_score_10fold\n",
        "                                      best_validation_rmse = average_val_rmse_score_10fold\n",
        "                                      best_lr = lr\n",
        "                                      best_batchsize = batch_size\n",
        "                                      best_loss_func = type_loss_func\n",
        "                                      best_type_optimizer = type_optimizer\n",
        "                                      best_neural_network_structure = neural_network_structure\n",
        "    print(\"The best hyperparameters:\\n\") \n",
        "    print(\"learning rate: %s, batch size: %s, type_loss_func: %s, type_optimizer: %s, neural_network_structure: %s, RMSE: %s\\n\" % (best_lr,best_batchsize,best_type_optimizer,best_neural_network_structure,best_validation_rmse))\n",
        "    end_time = time.clock()\n",
        "    print(\"The total training time is: \" + str(end_time-start_time))\n",
        "    # Test model\n",
        "    pred = (neural_network) \n",
        "    output=neural_network.eval({X_p: X_test}) \n",
        "\n",
        "    # plot prediction and labels\n",
        "    pyplot.plot(y_test[0:500], 'ro', label='Testing labels')\n",
        "    pyplot.plot(output[0:500], 'bo', label='Predictions')\n",
        "    pyplot.xlabel('Instances') \n",
        "    pyplot.ylabel('Average_log(PM2.5 + 1)')\n",
        "    pyplot.title('Partial Predictions and labels')\n",
        "    pyplot.show()\n",
        "\n",
        "    # final RMSE\n",
        "    rmse_score = sess.run(eval_RMSE,feed_dict={X_p:X_test,Y_p:y_test})\n",
        "    print(\"Average Test RMSE: \" + str(rmse_score))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fold 1, lr: 0.001, batch_size: 64, type_loss_func: MSE, type_optimizer: GradientDescent, neural_network_structure: multilayer_perceptron1\n",
            "Epoch: 1000, Training loss: 0.8382342, Validation loss: 1.234572, \n",
            "Epoch: 2000, Training loss: 1.0282356, Validation loss: 0.9523863, \n",
            "Epoch: 3000, Training loss: 1.0211468, Validation loss: 0.8487048, \n",
            "Epoch: 4000, Training loss: 0.6997118, Validation loss: 0.7942722, \n",
            "Epoch: 5000, Training loss: 0.8730383, Validation loss: 0.7565574, \n",
            "Epoch: 6000, Training loss: 0.72345614, Validation loss: 0.7358815, \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-b6d91c2756cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX_p\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_p\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                                 \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX_p\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_p\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                                 \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mselect_epoch_from\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m     \"\"\"\n\u001b[0;32m--> 798\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mexperimental_ref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   5405\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5406\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 5407\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}